testt
# ============================================
# BEGINNING OF CAREERS DEMO PIPELINE (PySpark)
# Using real IDs: aoid, ooid, employee_guid
# ============================================

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# ------------------------------------------------
# 0. ASSUMPTIONS ABOUT YOUR BASE TABLE (df_raw)
# ------------------------------------------------
# df_raw has (at least):
#   aoid              : person id
#   ooid              : company id
#   employee_guid     : person–company spell id
#   month_start       : first day of pay month (date)
#   dob               : date of birth
#   gross_wages       : total wages paid in that month
#   job_title         : raw title
#   job_cluster       : title cluster (20–30 groups)  [optional but recommended]
#   industry_map      : industry cluster / code
#   fulltime_parttime : 'F' / 'P' (or similar)
#   status_flag       : 'A' = active, 'T' = terminated
#
# Optional: if you already have tenure_months, you can reuse it instead of recomputing.

df = df_raw

# ------------------------------------------------
# 1. FILTER UNIVERSE: AGES 18–25, BASIC CLEANING
# ------------------------------------------------

df = df.withColumn(
    "age",
    F.floor(F.months_between(F.col("month_start"), F.col("dob")) / 12)
)

df = (
    df
    .filter((F.col("age") >= 18) & (F.col("age") <= 25))
    .filter(F.col("gross_wages") > 0)          # drop zero/negative pay
)

# ------------------------------------------------
# 2. ADD QUARTER + BASIC TIME KEYS
# ------------------------------------------------

df = (
    df
    .withColumn("year",   F.year("month_start"))
    .withColumn("q",      F.quarter("month_start"))
    .withColumn("year_q_index", F.col("year") * 4 + F.col("q"))  # numeric order
    .withColumn("quarter_label", F.concat_ws(" Q", F.col("year"), F.col("q")))
)

# ------------------------------------------------
# 3. TENURE & NEW HIRE / INCUMBENT AT FIRM LEVEL
#    (spell = employee_guid)
# ------------------------------------------------

w_spell = Window.partitionBy("employee_guid")

df = (
    df
    .withColumn("first_month_at_firm", F.min("month_start").over(w_spell))
    .withColumn(
        "tenure_months",
        F.floor(F.months_between(F.col("month_start"), F.col("first_month_at_firm")))
    )
    # New hire month at that firm = first month of the spell
    .withColumn(
        "is_new_hire_month",
        (F.col("month_start") == F.col("first_month_at_firm")).cast("int")
    )
    # Incumbent = tenure > 0
    .withColumn(
        "is_incumbent_month",
        (F.col("tenure_months") > 0).cast("int")
    )
)

# ------------------------------------------------
# 4. BUILD WORKER–QUARTER TABLE (spell-based)
#    ONE ROW PER employee_guid PER QUARTER
# ------------------------------------------------

df_worker_q = (
    df
    .groupBy(
        "employee_guid",        # spell
        "aoid",                 # person
        "ooid",                 # company
        "industry_map",
        "job_title",
        "job_cluster",
        "fulltime_parttime",
        "year",
        "q",
        "year_q_index"
    )
    .agg(
        F.sum("gross_wages").alias("total_wages_q"),
        F.avg("gross_wages").alias("avg_wages_q"),
        F.min("month_start").alias("first_month_in_q"),
        F.max("month_start").alias("last_month_in_q"),
        F.max("is_new_hire_month").alias("is_new_hire_q"),
        F.max("is_incumbent_month").alias("is_incumbent_q"),
        F.max(F.when(F.col("status_flag") == "A", 1).otherwise(0)).alias("is_active_q")
    )
)

# Overall hire cohort for each spell = first quarter we see that employee_guid
w_spell_q = Window.partitionBy("employee_guid")

df_worker_q = (
    df_worker_q
    .withColumn("hire_cohort_yq", F.min("year_q_index").over(w_spell_q))
    .withColumn(
        "is_hire_cohort_q",
        (F.col("year_q_index") == F.col("hire_cohort_yq")).cast("int")
    )
    .withColumn(
        "quarter_label",
        F.concat_ws(" Q", F.col("year"), F.col("q"))
    )
)

# ------------------------------------------------
# 5. QUARTERLY TOPLINE METRICS (EMPLOYMENT, PAY, HIRING)
#    WITH FT / PT BREAKOUTS
# ------------------------------------------------

# 5.1 Employment (headcount of ACTIVE spells)
base_active = df_worker_q.filter(F.col("is_active_q") == 1)

quarterly_employment = (
    base_active
    .groupBy("year", "q", "quarter_label")
    .agg(
        F.countDistinct("employee_guid").alias("headcount_18_25"),
        F.countDistinct(F.when(F.col("fulltime_parttime") == "F", F.col("employee_guid"))).alias("headcount_ft"),
        F.countDistinct(F.when(F.col("fulltime_parttime") == "P", F.col("employee_guid"))).alias("headcount_pt"),
    )
)

# 5.2 Pay metrics (active spells)
quarterly_pay = (
    base_active
    .groupBy("year", "q", "quarter_label")
    .agg(
        F.expr("percentile_approx(avg_wages_q, 0.5)").alias("median_pay_18_25"),
        F.expr("percentile_approx(avg_wages_q, 0.1)").alias("p10_pay"),
        F.expr("percentile_approx(avg_wages_q, 0.25)").alias("p25_pay"),
        F.expr("percentile_approx(avg_wages_q, 0.75)").alias("p75_pay"),
        F.expr("percentile_approx(avg_wages_q, 0.9)").alias("p90_pay"),
        # FT / PT median pay (demo – can be refined)
        F.expr("percentile_approx(IF(fulltime_parttime='F', avg_wages_q, NULL), 0.5)").alias("median_pay_ft"),
        F.expr("percentile_approx(IF(fulltime_parttime='P', avg_wages_q, NULL), 0.5)").alias("median_pay_pt"),
    )
)

# 5.3 New hire counts (hire cohort quarter for each spell)
quarterly_hiring = (
    df_worker_q
    .filter(F.col("is_hire_cohort_q") == 1)
    .groupBy("year", "q", "quarter_label")
    .agg(
        F.countDistinct("employee_guid").alias("new_hires_18_25")
    )
)

# 5.4 Join topline + QoQ / YoY
quarterly_topline = (
    quarterly_employment
    .join(quarterly_pay, ["year", "q", "quarter_label"], "left")
    .join(quarterly_hiring, ["year", "q", "quarter_label"], "left")
    .orderBy("year", "q")
)

w_q = Window.orderBy("year", "q")

quarterly_topline = (
    quarterly_topline
    .withColumn(
        "headcount_qoq",
        (F.col("headcount_18_25") - F.lag("headcount_18_25", 1).over(w_q))
        / F.lag("headcount_18_25", 1).over(w_q)
    )
    .withColumn(
        "headcount_yoy",
        (F.col("headcount_18_25") - F.lag("headcount_18_25", 4).over(w_q))
        / F.lag("headcount_18_25", 4).over(w_q)
    )
    .withColumn(
        "median_pay_qoq",
        (F.col("median_pay_18_25") - F.lag("median_pay_18_25", 1).over(w_q))
        / F.lag("median_pay_18_25", 1).over(w_q)
    )
    .withColumn(
        "median_pay_yoy",
        (F.col("median_pay_18_25") - F.lag("median_pay_18_25", 4).over(w_q))
        / F.lag("median_pay_18_25", 4).over(w_q)
    )
)

# ------------------------------------------------
# 6. JOB ENTRY PATTERNS (FIRST JOB AT A FIRM)
# ------------------------------------------------

# First job spell = hire cohort quarter for each employee_guid
first_jobs = df_worker_q.filter(F.col("is_hire_cohort_q") == 1)

# 6.1 Top entry industries
entry_industries = (
    first_jobs
    .groupBy("industry_map")
    .agg(F.countDistinct("employee_guid").alias("entry_count"))
)

total_entries = entry_industries.agg(F.sum("entry_count").alias("total")).collect()[0]["total"]

entry_industries = entry_industries.withColumn(
    "entry_share",
    F.col("entry_count") / F.lit(total_entries)
)

# 6.2 Top entry job clusters
entry_job_clusters = (
    first_jobs
    .groupBy("job_cluster")
    .agg(F.countDistinct("employee_guid").alias("entry_count"))
    .orderBy(F.desc("entry_count"))
)

# 6.3 First-job tenure at first firm (spell-level)
first_employer = first_jobs.select(
    "employee_guid",
    "aoid",
    "ooid",
    "hire_cohort_yq"
).withColumnRenamed("ooid", "first_ooid")

df_with_first_company = df_worker_q.join(
    first_employer.select("employee_guid", "first_ooid", "hire_cohort_yq"),
    "employee_guid",
    "left"
)

# Keep only quarters where the spell is still at that first firm (it always is for employee_guid,
# but this structure lets you extend if you ever change the ID definition).
df_first_company_spells = df_with_first_company.filter(
    F.col("ooid") == F.col("first_ooid")
)

w_first_comp = Window.partitionBy("employee_guid")

df_first_company_spells = df_first_company_spells.withColumn(
    "last_yq_at_first_company",
    F.max("year_q_index").over(w_first_comp)
)

first_job_tenure = (
    df_first_company_spells
    .select("employee_guid", "hire_cohort_yq", "last_yq_at_first_company")
    .dropDuplicates(["employee_guid", "hire_cohort_yq", "last_yq_at_first_company"])
    .withColumn(
        "tenure_quarters",
        F.col("last_yq_at_first_company") - F.col("hire_cohort_yq") + F.lit(1)
    )
)

first_job_tenure_summary = (
    first_job_tenure
    .agg(
        F.expr("percentile_approx(tenure_quarters, 0.5)").alias("median_first_job_tenure_q"),
        F.expr("percentile_approx(tenure_quarters, 0.25)").alias("p25_first_job_tenure_q"),
        F.expr("percentile_approx(tenure_quarters, 0.75)").alias("p75_first_job_tenure_q"),
    )
)

# ------------------------------------------------
# 7. MOBILITY METRICS (JOB SWITCHING & PAY MOBILITY)
#    PERSON-LEVEL (aoid)
# ------------------------------------------------

w_mob = Window.partitionBy("aoid").orderBy("year_q_index")

df_mob = (
    df_worker_q
    .withColumn("prev_ooid",          F.lag("ooid").over(w_mob))
    .withColumn("prev_industry_map",  F.lag("industry_map").over(w_mob))
    .withColumn("prev_total_wages_q", F.lag("total_wages_q").over(w_mob))
)

df_mob = (
    df_mob
    .withColumn(
        "is_switch_firm",
        F.when(F.col("prev_ooid").isNull(), 0)
         .when(F.col("prev_ooid") != F.col("ooid"), 1)
         .otherwise(0)
    )
    .withColumn(
        "is_switch_industry",
        F.when(F.col("prev_industry_map").isNull(), 0)
         .when(F.col("prev_industry_map") != F.col("industry_map"), 1)
         .otherwise(0)
    )
    .withColumn(
        "pay_change",
        F.col("total_wages_q") - F.col("prev_total_wages_q")
    )
    .withColumn(
        "pay_change_up",
        F.when(F.col("prev_total_wages_q").isNull(), None)
         .when(F.col("pay_change") > 0, 1.0)
         .otherwise(0.0)
    )
)

mobility_summary = (
    df_mob
    .filter(F.col("prev_ooid").isNotNull())
    .agg(
        F.sum("is_switch_firm").alias("num_firm_switches"),
        F.sum("is_switch_industry").alias("num_industry_switches"),
        F.countDistinct("aoid").alias("num_people_with_transitions"),
        F.count("*").alias("num_quarter_transitions"),
        F.avg("pay_change_up").alias("share_moves_up_pay")
    )
)

# ------------------------------------------------
# 8. INDUSTRY-TO-INDUSTRY FLOW MATRIX
# ------------------------------------------------

flows = df_mob.filter(
    (F.col("prev_industry_map").isNotNull()) &
    (F.col("industry_map").isNotNull())
)

industry_flows = (
    flows
    .groupBy("prev_industry_map", "industry_map")
    .agg(F.countDistinct("aoid").alias("num_workers_moved"))
)

w_origin = Window.partitionBy("prev_industry_map")

industry_flows = (
    industry_flows
    .withColumn("origin_total", F.sum("num_workers_moved").over(w_origin))
    .withColumn("flow_share", F.col("num_workers_moved") / F.col("origin_total"))
)

# ------------------------------------------------
# 9. RETENTION METRICS (COHORT-BASED, 1Y DEMO)
#    SPELL-LEVEL RETENTION FROM HIRE COHORT
# ------------------------------------------------

df_ret = df_worker_q.select("employee_guid", "hire_cohort_yq", "year_q_index")

hire_quarter = (
    df_ret
    .filter(F.col("year_q_index") == F.col("hire_cohort_yq"))
    .select("employee_guid", "hire_cohort_yq")
    .dropDuplicates()
)

def retention_at_offset(df_ret, hire_quarter, k):
    target = (
        hire_quarter
        .withColumn("target_yq", F.col("hire_cohort_yq") + F.lit(k))
    )

    present = (
        target.alias("t")
        .join(
            df_ret.select("employee_guid", "year_q_index").dropDuplicates().alias("r"),
            (F.col("t.employee_guid") == F.col("r.employee_guid")) &
            (F.col("t.target_yq") == F.col("r.year_q_index")),
            "inner"
        )
    )

    total_cohort = hire_quarter.count()
    retained = present.count()
    return retained, total_cohort

ret_1q, cohort_size = retention_at_offset(df_ret, hire_quarter, k=1)
ret_2q, _          = retention_at_offset(df_ret, hire_quarter, k=2)
ret_4q, _          = retention_at_offset(df_ret, hire_quarter, k=4)

retention_summary = spark.createDataFrame(
    [
        ("1_quarter",  ret_1q / cohort_size if cohort_size > 0 else None),
        ("2_quarters", ret_2q / cohort_size if cohort_size > 0 else None),
        ("4_quarters", ret_4q / cohort_size if cohort_size > 0 else None),
    ],
    ["horizon", "retention_rate"]
)

# ============================================
# KEY OUTPUT TABLES FOR THE DEMO REPORT
# ============================================
#   quarterly_topline        : headcount, FT/PT, pay, new hires, QoQ/YoY
#   entry_industries         : top entry industries + shares
#   entry_job_clusters       : top entry job clusters
#   first_job_tenure_summary : first-job tenure distribution (quarters)
#   mobility_summary         : firm/industry switches, pay mobility
#   industry_flows           : industry-to-industry flow matrix
#   retention_summary        : 1, 2, 4 quarter retention rates


